%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{varwidth}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
% 			\vspace{#2}
% 			\begin{center}
% 			Figure \thelecnum.#1:~#3
% 			\end{center}
% 	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
% \newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{21}{Feature/Data Selection and Greedy Algorithm}{Abir De}{Group 2}
%\lecture{x}{Title}{Abir De}{Group y}

\section{Feature/Data Selection}
% The following topics are important for machine learning:
% \begin{enumerate}
%     \item Conditional Independence or probabilities, Bayesian relationship
%     \item Introducing Random Variables: Bernoulli, Normal, Poisson
%     \item Expectation, Variance, Covariance
%     \item Central Limit Theorem, Law of large numbers
% \end{enumerate}

\subsection{Feature Selection}
Feature selection is a technique for limiting the input variable to your model by only using useful data and eliminating noise. It's the process of selecting appropriate characteristics for your machine learning model based on the sort of problem you're attempting to answer automatically.

\begin{figure}[H]

\centering
\includegraphics[scale =0.92]{Feature Selection.png}
\end{figure}

% Suppose you have a set of \textbf{Data}. This can be
% \begin{itemize}
%     \item A set of images $\xrightarrow{m_\theta}$ identify the objects
%     \item A set of paragraphs $\xrightarrow{m_\theta}$ identify the topics 
% \end{itemize}
% where $m_\theta$ denotes a model with parameters $\theta$. The objective is to devise a model which learns to do a task. The following question can be posed:
% \begin{flushleft}
% Q. \textbf{What is the underlying thesis/concept behind the hope that it will be possible to train the model $m_\theta$ to get an accurate output?}\\

% Ans. \color{blue}
% \textbf{The law of large numbers}
% \end{flushleft}

\subsection{Feature Selection Models}
They are of 2 types:
\begin{itemize}
    \item[1.] \textbf{Supervised Models} - The method of supervised feature selection uses the output label class for feature selection. They use the target variables to identify variables that can improve the model's efficiency. The supervised models are further divided into 3:
    \begin{itemize}
        \item[a)] \textbf{Filter Method:} Features are discarded based on their relationship to the output, or how they correlate with the outcome.  Correlation is used to see if the features are positively or negatively correlated to the output labels and drop features accordingly. Eg: Information Gain, Fisher’s Score, etc
        \begin{figure}[H]
        \centering
        \includegraphics[scale =0.9]{Filter Method Flowchart.png}
        \caption{Filter Method Flowchart}
        \end{figure}
        \item[b)] \textbf{Wrapper Method:} We divide our data into subsets and used this to train a model. We add and eliminate features based on the model's output and retrain the model. It uses a greedy approach to create subsets and analyses the accuracy of all potential feature combinations. Eg: Forward Selection, Backwards Elimination, etc.
        \begin{figure}[H]
        \centering
        \includegraphics[scale =0.9]{Wrapper Method Flowchart.png}
        \caption{Wrapper Method Flowchart}
        \end{figure}
        \item[c)] \textbf{Intrinsic Method:} To construct the best subset, this method incorporates the benefits of both the Filter and Wrapper methods. This approach takes care of the iterative machine training process while keeping the computation cost low. Eg: Lasso and Ridge Regression.
        \begin{figure}[H]
        \centering
        \includegraphics[scale =0.9]{Intrinsic Method Flowchart.png}
        \caption{Intrinsic Method Flowchart}
        \end{figure}
    \end{itemize}
    \item[2.] \textbf{Unsupervised Models} - Unsupervised feature selection is a method for feature selection that does not require the output label class. They're what we utilise for unlabeled data.
\end{itemize}




\subsection{Data Selection}
A machine learning solution's success is dependent on the quality, quantity, preparation, and selection of data.

Avoiding \textbf{selection bias} is the first step toward success. When the samples used to create the model are not fully representative of the scenarios that the model may be used for in the future, especially with fresh and unseen data, selection bias occurs.

Missing values, useless values (e.g., NA), outliers, and other irregularities are common in data. Raw data must be parsed, cleaned, converted, and pre-processed before modelling and analysis can begin. This is commonly known as \textbf{data munging} or \textbf{data wrangling.}
% Suppose that we have $N$ samples $X_i \sim f(\cdot), \:\:\: i = 1, \hdots, N$ drawn from an unknown distribution. Most of the times, the goal of machine learning is to estimate the distribution $f$ or its properties. For example, in the task of generating images (GAN, VAE etc), the objective of the machine learning model is to identify the underlying distribution of the data samples. Being able to learn the underlying distribution is important even for tasks like housing price prediction, image classification etc because if a test sample is sampled from a different distribution is given to the model, the model will likely fail and give incorrect results. 
% \begin{flushleft}
% Q. \textbf{What is the least that can be found about $f$ using $X_1, \hdots, X_N$?}\\

% Ans. \color{blue} Central Limit Theorem states that as long as $N$ is large, the $$\mathbb{E}_{X \sim f(\cdot)}[X] = \frac{\sum X_i}{N}$$
% The Law of Large Numbers further states that
% $$\mathbb{E}_{X \sim f(\cdot)}[X] \xrightarrow{}\frac{\sum X_i}{N}$$
% as $N \xrightarrow{} \infty$. Moreover, the random variable $Z_N = \frac{\sum X_i}{N}$ follows the normal distribution $\mathcal{N}(\mathbb{E}[X], \sigma^2)$
% with $\sigma^2 \propto \frac{1}{N}$ with
% $$\lim \limits_{N \xrightarrow{}\infty}\mathbb{E}[Z_N] = \mathbb{E}[X]$$
% \end{flushleft}
% \begin{flushleft}
% Q. \textbf{What else do we need to completely characterize $f$?}\\

% Ans. \color{blue} \textbf{We need the higher order moments!} Using these the moment generating function can be obtained.
% \end{flushleft}

% \definition{The moment generating function (MGF) $F(s)$ for a random variable $X$ is defined as the Laplace Transform of the probability density function (PDF) $f(x)$. The inverse Laplace Transform of MGF will give the PDF.
% $$F(s) = \int \limits_{-\infty}^\infty e^{-sx}\, f(x)\, dx$$
% $$f(x) = \int \limits_{-\infty}^\infty e^{sx}\, F(s)\, ds$$}

% \proposition{Using only the moments, the MGF can be determined.}
% \begin{proof}
% \begin{eqnarray*}
% F(s) &=& \int \limits_{-\infty}^\infty e^{-sx}\, f(x)\, dx\\
% &=& \int \limits_{-\infty}^\infty \sum \limits_{k = 0}^{\infty} \frac{(-s)^k\, x^k}{k!}\, f(x)\, dx\\
% &=& \sum \limits_{k = 0}^{\infty} \frac{(-s)^k}{k!}\, \mathbb{E}[X^k]
% \end{eqnarray*}
% \begin{flushleft}
% Note: The discussion about the region of convergence of the Laplace transform is out of the scope of this course.
% \end{flushleft}
% \end{proof}

% \proposition{
% Using just the $N$ samples $X_i \sim f(\cdot), \:\:\: i = 1, \hdots, N$, the PDF $f$ can be estimated using the Law of Large Numbers.
% }
% \begin{proof}
% This can be proved as follows:
% \begin{enumerate}
%     \item All moments $\mathbb{E}[X^k]$ can be estimated by invoking the Law of Large Numbers
%     \item The MGF can be found by invoking the Claim 1.2 above.
%     \item $f$ can be estimated by taking the inverse Laplace Transform of the MGF
% \end{enumerate}
% \end{proof}

% \subsection{Practice Problems}
% \normalfont
% \begin{enumerate}
%     \item  A fair coin is tossed 5 times. Find  $\mathbb{P}(\#H > \# T)$\\
%     \color{blue}It is easy to see that as 5 is odd, there is not possibility of $\#H = \#T$. Hence $\mathbb{P}(\#H > \# T) = 1/2$
%     \color{black}
%     \item $X \sim f(\cdot), Y \sim g(\cdot)$. Then $Z = X+Y \sim \:?$\\
%     \color{blue} \begin{eqnarray*}
%     \mathbb{P}(Z \leq z) &=& \mathbb{P}(X+Y \leq z)\\
%                         &=& \int \limits_{-\infty}^\infty \int \limits_{-\infty}^{z-x}f(x) \, g(y) \, dx\,dy\\
%                         &=& \int \limits_{-\infty}^\infty f(x)\, G(z-x) \, dx
%     \end{eqnarray*}
%     On differentiating, we get
%     $$f_Z(z) = \int \limits_{-\infty}^\infty f(x) g(z-x)\,dx = (f * g)(z)$$
% \end{enumerate}



\section{Greedy Algorithm}

A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.
\subsection{Advantages of Greedy Algorithm}
\begin{itemize}
    \item It is a highly optimized and one of the most straightforward algorithms.
    \item This algorithm takes lesser time as compared to others because the best solution is immediately reachable.
    \item In the greedy method, multiple activities can execute in a given time frame.
    \item The algorithm is easy to implement.
\end{itemize}
\subsection{Disadvantages of Greedy Algorithm}
\begin{itemize}
    \item Sometimes greedy algorithms fail to find the globally optimal solution because they do not consider all the data.
\end{itemize}
% Solve the following problems:
% \begin{enumerate}
%     \item For two square matrices $A, B$of size $n\times n$ if $AB = BA$ for all B, then show that $A = cI_n$ for some $c \in \mathbb{R}$\\
%     \color{blue}
%     Pick $B$ to be a diagonal matrix with pair-wise distinct elements. Then it can be shown that $A$ is also a diagonal matrix. Now pick $B$ to be a matrix with all ones, i.e. $B = [1]_{ij}$. As $A$ is diagonal, $AB=BA$ implies all diagonal entries of $A$ are equal i.e., $A = cI_n$ for some $c \in \mathbb{R}$
%     \color{black}
%     \item If $x^\top A x = 0\:\: \forall x \in \mathbb{R}^n$ then show that $A$ is skew-symmetric.\\
%     \color{blue}
%     On differentiating the above equation we get
%     $$(A + A^\top)x = 0 \:\: \forall x \in \mathbb{R}^n$$
%     This implies $A = -A^\top$
%     \color{black}
%     \item Show that $\text{rank}(AB) \leq \text{rank}(A)$\\
%     \color{blue} Each column of $AB$ can be viewed as a linear combination of columns of $A$. Hence, if the dimension of column space of $A$ is $r$, the dimension of column space of $AB$ cannot be more than $r$. In other words,
%     $\text{rank}(AB) \leq \text{rank}(A)$
%     \color{black}
%     \item Suppose you have a uniform sampler which samples uniformly from $[0, 1]$. Propose an algorithm which uses this uniform sampler to generate samples from any given distribution.\\
%     \color{blue}
%     Suppose we have a uniform random variable $U \sim \text{Uniform}([0, 1])$. We need to find a function $g$ such that the PDF of the random variable $g(U)$ will be same as that of the given distribution, say $f$. That is $g(U) \sim f$. Which is same as
%     \begin{eqnarray*}
%     \mathbb{P}(g(U) \leq x) &=& \mathbb{P}(U \leq g^{-1}(x))\\
%     \implies \int \limits_{-\infty}^x f(x) \, dx &=& \int \limits_{-\infty}^{g^{-1}(x)} 1 \, du\\
%     \implies F(x) &=& g^{-1}(x)\\
%     \implies g &=& F^{-1}
%     \end{eqnarray*}
%     \color{black}
% \end{enumerate}
\section{Group Details and Individual Contribution}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
Name & Roll Number & Contribution \\
\hline
Aditya Byju & 20D070004 & 21.1.1 \& 21.1.2 \\ 
Devanshu Saraf & 200040042 & 21.1.3 \& 21.2 \\ 
\hline
\end{tabular}
\end{center}
\end{document}





